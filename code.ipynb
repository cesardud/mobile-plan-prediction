{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Modelo de predicción de plan (Smart o Ultra) para usuarios de Megaline.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   calls  minutes  messages   mb_used  is_ultra\n",
      "0   40.0   311.90      83.0  19915.42         0\n",
      "1   85.0   516.75      56.0  22696.96         0\n",
      "2   77.0   467.66      86.0  21060.45         0\n",
      "3  106.0   745.53      81.0   8437.39         1\n",
      "4   66.0   418.74       1.0  14502.75         0\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3214 entries, 0 to 3213\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   calls     3214 non-null   float64\n",
      " 1   minutes   3214 non-null   float64\n",
      " 2   messages  3214 non-null   float64\n",
      " 3   mb_used   3214 non-null   float64\n",
      " 4   is_ultra  3214 non-null   int64  \n",
      "dtypes: float64(4), int64(1)\n",
      "memory usage: 125.7 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Abrimos el fd, observamos que no existan observaciones faltantes y que los datos esten correctos\n",
    "df = pd.read_csv('users_behavior.csv')\n",
    "print(df.head())\n",
    "print()\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dividimos el df en variables dependientes e independientes\n",
    "x= df.drop(['is_ultra'], axis=1)\n",
    "y=df['is_ultra']\n",
    "#Segmentamos en entrenamiento, validacion y prueba\n",
    "x_train, x_temp, y_train, y_temp = train_test_split(x, y, test_size=0.40, random_state=12345)\n",
    "x_valid, x_test, y_valid, y_test = train_test_split(x_temp, y_temp, test_size = .5, random_state=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy del mejor modelo en el conjunto de validación (max_depth = 3): 0.7853810264385692\n",
      "\n",
      "Final accuracy en el conjunto de prueba: 0.7792\n",
      "\n",
      "Accuracy del DummyClassifier (estrategia = \"stratified\"): 0.5365\n"
     ]
    }
   ],
   "source": [
    "#Comenzamos con el modelo de Arboles de clasificación\n",
    "best_model_1 = None\n",
    "best_accuracy_1 = 0\n",
    "best_depth_1 = 0\n",
    "\n",
    "for depth in range(1, 11):\n",
    "    model_1 = DecisionTreeClassifier(max_depth= depth, random_state=12345)\n",
    "    model_1.fit(x_train, y_train)\n",
    "    predictions_1_valid = model_1.predict(x_valid)\n",
    "    accuracy_1 = accuracy_score(y_valid, predictions_1_valid)\n",
    "    if accuracy_1 > best_accuracy_1:\n",
    "        best_model_1=model_1\n",
    "        best_accuracy_1=accuracy_1\n",
    "        best_depth_1=depth\n",
    "        \n",
    "print(f\"Accuracy del mejor modelo en el conjunto de validación (max_depth = {best_depth_1}): {best_accuracy_1}\")        \n",
    "print()\n",
    "final_model_1 = DecisionTreeClassifier(max_depth=best_depth_1, random_state=12345)\n",
    "final_model_1.fit(x_train, y_train)\n",
    "predictions_1_test = final_model_1.predict(x_test)\n",
    "\n",
    "final_accuracy_1 = accuracy_score(y_test, predictions_1_test)\n",
    "print(f'Final accuracy en el conjunto de prueba: {final_accuracy_1:.4f}')\n",
    "\n",
    "print()\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy = DummyClassifier(strategy='stratified', random_state=12345)\n",
    "dummy.fit(x_train, y_train)\n",
    "dummy_predictions = dummy.predict(x_test)\n",
    "dummy_accuracy = accuracy_score(y_test, dummy_predictions)\n",
    "\n",
    "print(f'Accuracy del DummyClassifier (estrategia = \"stratified\"): {dummy_accuracy:.4f}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Árboles de clasificación\n",
    "\n",
    "Se hizo el bucle en el conjunto de entrenamiento para escoger el mejor hiperparametro depth, para este caso fué un max_depth=3, en donde se obtuvo un accuracy de .785.\n",
    "\n",
    "Al aplicar el modelo con max_depth=3 y comprobarlo en el conjunto de prueba, se obtuvo un accuracy de .779, lo que indica que el modelo acertó el plan del cliente el 77.9% de las veces.\n",
    "\n",
    "Se probó la cordura del modelo comparandola con una prueba dummy donde se haga un modelo sin aprendizaje donde se hacen predicciones al azar pero respetando la proporción original estrategia = \"stratified\". Dicho modelo tuvo un accuracy de .53, menor al modelo de aprendizaje DecisionTreeClassifier con depth=10; por lo que podemos decir que nuestro modelo final_model_1 no solo adivina; sino que tiene capacidad predictiva real.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy del mejor modelo en el conjunto de validación (n_estimators = 10): 0.7853810264385692\n",
      "\n",
      "Final accuracy en el conjunto de prueba: 0.7807\n"
     ]
    }
   ],
   "source": [
    "#Probamos ahora el modelo Bosque Aleatorio de Clasificación\n",
    "best_model_2 = None\n",
    "best_accuracy_2 = 0\n",
    "best_est_2 = 0\n",
    "for est in range(1,11):\n",
    "    model_2=RandomForestClassifier(random_state=12345, n_estimators= est)\n",
    "    model_2.fit(x_train, y_train)\n",
    "    predictions_2_valid = model_2.predict(x_valid)\n",
    "    accuracy_2 = accuracy_score(y_valid, predictions_2_valid)\n",
    "    if accuracy_2 > best_accuracy_2:\n",
    "        best_model_2=model_2\n",
    "        best_accuracy_2=accuracy_2\n",
    "        best_est_2=est\n",
    "print(f\"Accuracy del mejor modelo en el conjunto de validación (n_estimators = {best_est_2}): {best_accuracy_2}\")        \n",
    "print()\n",
    "final_model_2 = RandomForestClassifier(random_state=12345, n_estimators= best_est_2)\n",
    "final_model_2.fit(x_train, y_train)\n",
    "predictions_2_test = final_model_2.predict(x_test)\n",
    "\n",
    "final_accuracy_2 = accuracy_score(y_test, predictions_2_test)\n",
    "print(f'Final accuracy en el conjunto de prueba: {final_accuracy_2:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bosque aleatorio de clasificación\n",
    "\n",
    "Se hizo el bucle en el conjunto de entrenamiento para escoger el mejor hiperparametro n_estimators, para este caso fué un n_estimators=10, en donde se obtuvo un accuracy de .785.\n",
    "\n",
    "Al aplicar el modelo con n_estimators=10 y comprobarlo en el conjunto de prueba, se obtuvo un accuracy de .78, lo que indica que el modelo acertó el plan del cliente el 78% de las veces.\n",
    "\n",
    "Se probó la cordura del modelo comparandola con una prueba dummy donde se haga un modelo sin aprendizaje donde se hacen predicciones al azar pero respetando la proporción original estrategia = \"stratified\". Dicho modelo tuvo un accuracy .53, menor al modelo de aprendizaje DecisionTreeClassifier con depth=10; por lo que podemos decir que nuestro modelo final_model_2 no solo adivina; sino que tiene capacidad predictiva real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy del mejor modelo en el conjunto de validación (C = 1.0): 0.7589424572317263\n",
      "\n",
      "Final accuracy en el conjunto de prueba: 0.7403\n"
     ]
    }
   ],
   "source": [
    "#Probamos ahora el modelo Logistico\n",
    "best_model_3 = None\n",
    "best_accuracy_3 = 0\n",
    "best_C_3 = 0\n",
    "for c in np.arange(1.0, 11.0):\n",
    "    model_3=LogisticRegression(random_state=12345, penalty='l2', solver='liblinear', C=c)\n",
    "    model_3.fit(x_train, y_train)\n",
    "    predictions_3_valid = model_3.predict(x_valid)\n",
    "    accuracy_3 = accuracy_score(y_valid, predictions_3_valid)\n",
    "    if accuracy_3 > best_accuracy_3:\n",
    "        best_model_3=model_3\n",
    "        best_accuracy_3=accuracy_3\n",
    "        best_C_3=c\n",
    "print(f\"Accuracy del mejor modelo en el conjunto de validación (C = {best_C_3}): {best_accuracy_3}\")        \n",
    "print()\n",
    "final_model_3 = LogisticRegression(random_state=12345, penalty='l2', solver='liblinear', C=best_C_3)\n",
    "final_model_3.fit(x_train, y_train)\n",
    "predictions_3_test = final_model_3.predict(x_test)\n",
    "\n",
    "final_accuracy_3 = accuracy_score(y_test, predictions_3_test)\n",
    "print(f'Final accuracy en el conjunto de prueba: {final_accuracy_3:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bosque aleatorio de clasificación\n",
    "\n",
    "Se hizo el bucle en el conjunto de entrenamiento para escoger el mejor hiperparametro C y aplicando la penalización Ridge l2, para este caso fué un C=1.0, en donde se obtuvo un accuracy de .758.\n",
    "\n",
    "Al aplicar el modelo con C=1.0 y comprobarlo en el conjunto de prueba, se obtuvo un accuracy de .74, lo que indica que el modelo acertó el plan del cliente el 74% de las veces.\n",
    "\n",
    "Se probó la cordura del modelo comparandola con una prueba dummy donde se haga un modelo sin aprendizaje donde se hacen predicciones al azar pero respetando la proporción original estrategia = \"stratified\". Dicho modelo tuvo un accuracy de .53 menor al modelo de aprendizaje DecisionTreeClassifier con depth=10; por lo que podemos decir que nuestro modelo final_model_3 no solo adivina; sino que tiene capacidad predictiva real, aunque no llega a la exactitud deseada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy del modelo Árboles de clasificación: 0.7792\n",
      "\n",
      "Final Accuracy del modelo Bosque Aleatorio de Clasificación: 0.7807\n",
      "\n",
      "Final Accuracy del modelo Logístico: 0.7403\n"
     ]
    }
   ],
   "source": [
    "print(f'Final Accuracy del modelo Árboles de clasificación: {final_accuracy_1:.4f}')\n",
    "print()\n",
    "print(f'Final Accuracy del modelo Bosque Aleatorio de Clasificación: {final_accuracy_2:.4f}')\n",
    "print()\n",
    "print(f'Final Accuracy del modelo Logístico: {final_accuracy_3:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusiónes\n",
    "Posterior a hacer pruebas con 3 modelos con distintos hiperparámetros, todos los modelos aprobaron la cordura del modelo comparandola con una prueba dummy\n",
    "\n",
    "\n",
    "Se obtuvo que tanto los modelos Árboles de clasificación y Bosque Aleatorio de Clasificación obtuvieron un umbral de exactitud mayor al objetivo \">75\", mientras que el Accuracy del modelo Logístico se encuentra a niveles cercanos y podría ser aceptable en varios casos, en este al no alcanzar el objetivo se prefieren los otros 2.\n",
    "\n",
    "\n",
    "El Accuracy del modelo Árboles de clasificación: 0.7792 es menor al Accuracy del modelo Bosque Aleatorio de Clasificación: 0.7807; por lo que en este modelo con estas variables se preferiría el uso del modelo Bosque Aleatorio de Clasificación.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 298,
    "start_time": "2025-04-22T23:35:25.609Z"
   },
   {
    "duration": 205,
    "start_time": "2025-04-22T23:35:44.283Z"
   },
   {
    "duration": 15,
    "start_time": "2025-04-22T23:36:01.452Z"
   },
   {
    "duration": 21,
    "start_time": "2025-04-22T23:36:10.143Z"
   },
   {
    "duration": 16,
    "start_time": "2025-04-22T23:38:57.037Z"
   },
   {
    "duration": 505,
    "start_time": "2025-04-22T23:42:19.338Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-22T23:57:32.478Z"
   },
   {
    "duration": 83,
    "start_time": "2025-04-23T00:08:09.532Z"
   },
   {
    "duration": 9,
    "start_time": "2025-04-23T00:08:15.161Z"
   },
   {
    "duration": 8,
    "start_time": "2025-04-23T00:11:50.192Z"
   },
   {
    "duration": 16,
    "start_time": "2025-04-23T00:36:37.676Z"
   },
   {
    "duration": 32,
    "start_time": "2025-04-23T00:36:44.637Z"
   },
   {
    "duration": 24,
    "start_time": "2025-04-23T00:36:46.357Z"
   },
   {
    "duration": 53,
    "start_time": "2025-04-23T00:37:10.923Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-23T00:44:11.802Z"
   },
   {
    "duration": 54,
    "start_time": "2025-04-23T00:48:31.384Z"
   },
   {
    "duration": 159,
    "start_time": "2025-04-23T00:50:55.836Z"
   },
   {
    "duration": 86,
    "start_time": "2025-04-23T00:51:16.870Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-23T00:53:07.092Z"
   },
   {
    "duration": 59,
    "start_time": "2025-04-23T00:53:35.254Z"
   },
   {
    "duration": 59,
    "start_time": "2025-04-23T00:53:56.789Z"
   },
   {
    "duration": 62,
    "start_time": "2025-04-23T00:54:23.927Z"
   },
   {
    "duration": 61,
    "start_time": "2025-04-23T01:02:44.568Z"
   },
   {
    "duration": 59,
    "start_time": "2025-04-23T01:03:00.503Z"
   },
   {
    "duration": 66,
    "start_time": "2025-04-23T01:08:07.894Z"
   },
   {
    "duration": 64,
    "start_time": "2025-04-23T01:25:45.352Z"
   },
   {
    "duration": 64,
    "start_time": "2025-04-23T01:39:25.634Z"
   },
   {
    "duration": 65,
    "start_time": "2025-04-23T01:59:14.095Z"
   },
   {
    "duration": 65,
    "start_time": "2025-04-23T01:59:16.164Z"
   },
   {
    "duration": 235,
    "start_time": "2025-04-23T02:05:13.209Z"
   },
   {
    "duration": 233,
    "start_time": "2025-04-23T02:08:32.361Z"
   },
   {
    "duration": 61,
    "start_time": "2025-04-23T02:10:51.438Z"
   },
   {
    "duration": 63,
    "start_time": "2025-04-23T02:11:05.892Z"
   },
   {
    "duration": 236,
    "start_time": "2025-04-23T02:11:54.795Z"
   },
   {
    "duration": 885,
    "start_time": "2025-04-23T20:24:10.496Z"
   },
   {
    "duration": 94,
    "start_time": "2025-04-23T20:24:13.412Z"
   },
   {
    "duration": 7,
    "start_time": "2025-04-23T20:24:15.329Z"
   },
   {
    "duration": 74,
    "start_time": "2025-04-23T20:24:17.062Z"
   },
   {
    "duration": 271,
    "start_time": "2025-04-23T20:24:23.997Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-23T20:26:06.135Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-23T20:47:26.913Z"
   },
   {
    "duration": 329,
    "start_time": "2025-04-23T20:47:40.880Z"
   },
   {
    "duration": 69,
    "start_time": "2025-04-23T20:48:15.744Z"
   },
   {
    "duration": 8,
    "start_time": "2025-04-23T20:49:02.879Z"
   },
   {
    "duration": 62,
    "start_time": "2025-04-23T20:52:14.725Z"
   },
   {
    "duration": 61,
    "start_time": "2025-04-23T20:53:35.674Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-23T20:58:24.417Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-23T21:06:25.885Z"
   },
   {
    "duration": 15,
    "start_time": "2025-04-23T21:06:27.269Z"
   },
   {
    "duration": 5,
    "start_time": "2025-04-23T21:06:30.263Z"
   },
   {
    "duration": 65,
    "start_time": "2025-04-23T21:06:32.097Z"
   },
   {
    "duration": 239,
    "start_time": "2025-04-23T21:06:37.842Z"
   },
   {
    "duration": 70,
    "start_time": "2025-04-23T21:06:40.862Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-23T21:06:43.195Z"
   },
   {
    "duration": 828,
    "start_time": "2025-04-24T02:15:56.521Z"
   },
   {
    "duration": 24,
    "start_time": "2025-04-24T02:15:57.351Z"
   },
   {
    "duration": 6,
    "start_time": "2025-04-24T02:15:57.377Z"
   },
   {
    "duration": 70,
    "start_time": "2025-04-24T02:15:57.385Z"
   },
   {
    "duration": 241,
    "start_time": "2025-04-24T02:15:57.456Z"
   },
   {
    "duration": 71,
    "start_time": "2025-04-24T02:15:57.699Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-24T02:15:57.771Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
